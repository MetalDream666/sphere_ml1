{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import html2text\n",
    "import os\n",
    "import bs4\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28026\n"
     ]
    }
   ],
   "source": [
    "doc_to_title = {}\n",
    "stem = Mystem()\n",
    "with open('docs_titles.tsv') as f:\n",
    "    for num_line, line in enumerate(f):\n",
    "        if num_line == 0:\n",
    "            continue\n",
    "        data = line.strip().split('\\t', 1)\n",
    "        if len(data) > 1:\n",
    "            # print(data[1])\n",
    "            data[1] = ''.join(stem.lemmatize(data[1])).replace('\\n', '')\n",
    "            # print(data[1])\n",
    "        doc_id = int(data[0])\n",
    "        if len(data) == 1:\n",
    "            title = ''\n",
    "        else:\n",
    "            title = data[1]\n",
    "        doc_to_title[doc_id] = title\n",
    "        \n",
    "print (len(doc_to_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 18) (11690,) (11690,)\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('train_groups.csv')\n",
    "traingroups_titledata = {}\n",
    "for i in range(len(train_data)):\n",
    "    new_doc = train_data.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    target = new_doc['target']\n",
    "    title = doc_to_title[doc_id]\n",
    "    if doc_group not in traingroups_titledata:\n",
    "        traingroups_titledata[doc_group] = []\n",
    "    traingroups_titledata[doc_group].append((doc_id, title, target))\n",
    "\n",
    "y_train_lin = []\n",
    "X_train_lin = []\n",
    "groups_train = []\n",
    "mean = []\n",
    "median = []\n",
    "std = []\n",
    "\n",
    "for new_group in traingroups_titledata:\n",
    "    docs = traingroups_titledata[new_group]\n",
    "    for k, (doc_id, title, target_id) in enumerate(docs):\n",
    "        y_train_lin.append(target_id)\n",
    "        groups_train.append(new_group)\n",
    "        all_dist = []\n",
    "        words = set(title.strip().split())\n",
    "        for j in range(0, len(docs)):\n",
    "            if k == j:\n",
    "                continue\n",
    "            doc_id_j, title_j, target_j = docs[j]\n",
    "            words_j = set(title_j.strip().split())\n",
    "            all_dist.append(len(words.intersection(words_j)))\n",
    "            \n",
    "        all_dist = sorted(all_dist, reverse=True)\n",
    "        mean.append(np.mean(np.array(all_dist)))\n",
    "        median.append(np.median(np.array(all_dist)))\n",
    "        std.append(np.std(np.array(all_dist)))\n",
    "        X_train_lin.append(all_dist[0:15])\n",
    "        \n",
    "X_train_lin = np.array(X_train_lin)\n",
    "X_train_lin = np.hstack((X_train_lin, np.array(mean)[:, np.newaxis]))\n",
    "X_train_lin = np.hstack((X_train_lin, np.array(median)[:, np.newaxis]))\n",
    "X_train_lin = np.hstack((X_train_lin, np.array(std)[:, np.newaxis]))\n",
    "y_train_lin = np.array(y_train_lin)\n",
    "groups_train = np.array(groups_train)\n",
    "print (X_train_lin.shape, y_train_lin.shape, groups_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16627, 18) (16627,) 16627\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nscaler = StandardScaler()\\nX_train_lin = scaler.fit_transform(X_train_lin)\\nscaler = StandardScaler()\\nX_test_lin = scaler.fit_transform(X_test_lin)\\n'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('test_groups.csv')\n",
    "testgroups_titledata = {}\n",
    "doc_id_test = []\n",
    "for i in range(len(test_data)):\n",
    "    new_doc = test_data.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    doc_id_test.append(new_doc['pair_id'])\n",
    "    title = doc_to_title[doc_id]\n",
    "    if doc_group not in testgroups_titledata:\n",
    "        testgroups_titledata[doc_group] = []\n",
    "    testgroups_titledata[doc_group].append((doc_id, title))\n",
    "\n",
    "X_test_lin = []\n",
    "groups_test = []\n",
    "mean = []\n",
    "median = []\n",
    "std = []\n",
    "\n",
    "for new_group in testgroups_titledata:\n",
    "    docs = testgroups_titledata[new_group]\n",
    "    for k, (doc_id, title) in enumerate(docs):\n",
    "        groups_test.append(new_group)\n",
    "        all_dist = []\n",
    "        words = set(title.strip().split())\n",
    "        for j in range(0, len(docs)):\n",
    "            if k == j:\n",
    "                continue\n",
    "            doc_id_j, title_j = docs[j]\n",
    "            words_j = set(title_j.strip().split())\n",
    "            all_dist.append(len(words.intersection(words_j)))\n",
    "        \n",
    "        all_dist = sorted(all_dist, reverse=True)\n",
    "        mean.append(np.mean(np.array(all_dist)))\n",
    "        median.append(np.median(np.array(all_dist)))\n",
    "        std.append(np.std(np.array(all_dist)))\n",
    "        X_test_lin.append(all_dist[0:15])\n",
    "        \n",
    "X_test_lin = np.array(X_test_lin)\n",
    "X_test_lin = np.hstack((X_test_lin, np.array(mean)[:, np.newaxis]))\n",
    "X_test_lin = np.hstack((X_test_lin, np.array(median)[:, np.newaxis]))\n",
    "X_test_lin = np.hstack((X_test_lin, np.array(std)[:, np.newaxis]))\n",
    "groups_test = np.array(groups_test)\n",
    "print (X_test_lin.shape, groups_test.shape, len(doc_id_test))\n",
    "\n",
    "'''\n",
    "scaler = StandardScaler()\n",
    "X_train_lin = scaler.fit_transform(X_train_lin)\n",
    "scaler = StandardScaler()\n",
    "X_test_lin = scaler.fit_transform(X_test_lin)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "\n",
    "\n",
    "m = Mystem()\n",
    "\n",
    "def get_normalized(text):\n",
    "    norm = list()\n",
    "    lemmas = m.lemmatize(text)\n",
    "    for lemma in lemmas:\n",
    "        if lemma.isalnum():\n",
    "            norm.append(lemma)\n",
    "    return norm\n",
    "\n",
    "\n",
    "def write_normalized(directory, name, normalized):\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)\n",
    "    if not directory.endswith('/'):\n",
    "        directory += '/'\n",
    "    with open(directory + name, 'w') as f:\n",
    "        for line in normalized:\n",
    "            f.write(line + '\\n')\n",
    "\n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "def normalize_html_files(directory):\n",
    "    for r, d, f in os.walk(directory):\n",
    "        for file in f:\n",
    "            with open(directory + file) as html:\n",
    "                link = html.readline()[:-1]\n",
    "                name = os.path.basename(html.name)\n",
    "                html_data = html.read()\n",
    "            text = text_from_html(html_data)\n",
    "            text = text.encode('utf-8', errors='surrogatepass').decode('utf-8', 'replace')\n",
    "            normalized = get_normalized(text)\n",
    "            # normalized.insert(0, link)\n",
    "            write_normalized('normalized/', name, normalized)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize_html_files('content/')\n",
    "# do not touch!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(directory, traingroups_titledata):\n",
    "    df_dict = {}\n",
    "    \n",
    "    for key in traingroups_titledata:\n",
    "        df_dict[key] = {}\n",
    "        count = 0\n",
    "        for data in traingroups_titledata[key]:\n",
    "            count += 1\n",
    "            with open(directory + str(data[0]) + '.dat', 'r') as file:\n",
    "                s = set()\n",
    "                for line in file:\n",
    "                    line = line[:-1]\n",
    "                    s.update([line])\n",
    "\n",
    "                for line in s:\n",
    "                    if len(line) <= 2:\n",
    "                        continue\n",
    "                    elif line in df_dict[key]:\n",
    "                        df_dict[key][line] += 1\n",
    "\n",
    "                    else:\n",
    "                        df_dict[key][line] = 1\n",
    "                            \n",
    "        for elem in df_dict[key]:            \n",
    "            df_dict[key][elem] = count / df_dict[key][elem]\n",
    "            df_dict[key][elem] = np.log(df_dict[key][elem])\n",
    "            \n",
    "                        \n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_components(directory, traingroups_titledata):\n",
    "    vector_set = {}\n",
    "    the_dict = {}\n",
    "    \n",
    "    for key in traingroups_titledata:\n",
    "        the_dict[key] = {}\n",
    "        for data in traingroups_titledata[key]:\n",
    "           # if data[2] == 1:\n",
    "            with open(directory + str(data[0]) + '.dat', 'r') as file:\n",
    "                for line in file:\n",
    "                    line = line[:-1]\n",
    "                    if len(line) <= 2:\n",
    "                        continue\n",
    "                    elif line in the_dict[key]:\n",
    "                        the_dict[key][line] += 1\n",
    "                    else:\n",
    "                        the_dict[key][line] = 1\n",
    "        #print(the_dict[key])\n",
    "                            \n",
    "    for key in the_dict:\n",
    "        l = []\n",
    "        for elem in the_dict[key]:\n",
    "            l.append((the_dict[key][elem], elem))\n",
    "            \n",
    "        l = list(reversed(sorted(l)))\n",
    "        length = len(l)\n",
    "        first = 0 #length // 100\n",
    "        second = length - length // 5\n",
    "        \n",
    "        l = l[first:second]\n",
    "        ll = []\n",
    "        for el in l:\n",
    "            if el[0] <= 5:\n",
    "                break\n",
    "            ll.append(el[1])\n",
    "        \n",
    "        s = set(ll)\n",
    "        vector_set[key] = {a: i for a, i in zip(ll, range(len(ll)))}\n",
    "                            \n",
    "    return vector_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tfidf_by_groups(directory, traingroups_titledata, idf_dictionary, vector_for_tfidf):\n",
    "    tfidf_dict = {} \n",
    "    words_lin = {}\n",
    "    \n",
    "    for key in traingroups_titledata: # by groups\n",
    "        dim1 = len(traingroups_titledata[key])\n",
    "        dim2 = len(vector_for_tfidf[key])\n",
    "        l = np.zeros(dim1)\n",
    "        matr = np.zeros((dim1, dim2))\n",
    "        words = {}\n",
    "        words_lin[key] = {}\n",
    "        group_len = 0\n",
    "        \n",
    "        for i, elem in enumerate(traingroups_titledata[key]): # by tuple (doc_id, title, target)\n",
    "            l[i] = elem[0]\n",
    "            words_doc = set()\n",
    "            all_dists = []\n",
    "            count = 0\n",
    "            \n",
    "            with open(directory + str(elem[0]) + '.dat', \"r\") as file: # by matr string ~ document tf-idf\n",
    "                for line in file:\n",
    "                    count += 1\n",
    "                    line = line[:-1]\n",
    "                    if line in vector_for_tfidf[key]:\n",
    "                        ind = vector_for_tfidf[key][line]\n",
    "                        matr[i][ind] += 1\n",
    "                        words_doc.update([line])\n",
    "                        \n",
    "            \n",
    "            group_len += count            \n",
    "            words[elem[0]] = words_doc\n",
    "\n",
    "            if count != 0:\n",
    "                matr[i] /= count\n",
    "\n",
    "            for j, word in enumerate(vector_for_tfidf[key]):\n",
    "                matr[i][j] *= idf_dictionary[key][word]\n",
    "            \n",
    "        for doc_id in words:\n",
    "            all_dist = []\n",
    "            for doc_id_second in words:\n",
    "                if doc_id == doc_id_second:\n",
    "                    continue\n",
    "                all_dist.append(len(words[doc_id].intersection(words[doc_id_second])))\n",
    "            all_dist = np.array(all_dist)\n",
    "            all_dist = all_dist / group_len\n",
    "            words_lin[key][doc_id] = sorted(all_dist, reverse=True)[0:20]\n",
    "        \n",
    "        \n",
    "                    \n",
    "        tfidf_dict[key] = (l, matr)\n",
    "    \n",
    "    return tfidf_dict, words_lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def cos_metric(tfidf_dict, traingroups_titledata):\n",
    "    ans = {}\n",
    "    \n",
    "    for key in tfidf_dict:\n",
    "        group_sum = np.zeros(tfidf_dict[key][1].shape[1])\n",
    "        for i, elem in enumerate(traingroups_titledata[key]):\n",
    "            vec = tfidf_dict[key][1][np.argwhere(tfidf_dict[key][0] == elem[0])[0][0]]\n",
    "            group_sum += vec\n",
    "        \n",
    "        cos_closest = metrics.pairwise.cosine_distances(tfidf_dict[key][1], tfidf_dict[key][1])\n",
    "        \n",
    "        ans[key] = {}\n",
    "        for elem in tfidf_dict[key][0]:\n",
    "            doc_id = elem\n",
    "            ans1 = 0\n",
    "            ans2 = 0\n",
    "            ans3 = 0\n",
    "            ans4 = 0\n",
    "            ans5 = 0\n",
    "            ans6 = 0\n",
    "            \n",
    "            vec = tfidf_dict[key][1][np.argwhere(tfidf_dict[key][0] == elem)[0][0]]\n",
    "            \n",
    "            ans1 = metrics.pairwise.cosine_distances([group_sum], [vec])[0][0]\n",
    "                \n",
    "            ans2 = np.min(cos_closest[np.nonzero(cos_closest)])\n",
    "            ans3 = np.max(cos_closest[np.nonzero(cos_closest)])\n",
    "            ans4 = np.mean(cos_closest[np.nonzero(cos_closest)])\n",
    "            ans5 = np.median(cos_closest[np.nonzero(cos_closest)])\n",
    "            \n",
    "            ans[key][doc_id] = (ans1, ans2, ans3, ans4, ans5)\n",
    "        \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tfidf_by_groups_for_headers(traingroups_titledata):\n",
    "    df_dict = {}\n",
    "    \n",
    "    for key in traingroups_titledata:\n",
    "        df_dict[key] = {}\n",
    "        count = 0\n",
    "        for data in traingroups_titledata[key]:\n",
    "            count += 1\n",
    "            s = set()\n",
    "            for line in data[1].split():\n",
    "                s.update([line])\n",
    "\n",
    "            for line in s:\n",
    "                if len(line) <= 2:\n",
    "                    continue\n",
    "                elif line in df_dict[key]:\n",
    "                    df_dict[key][line] += 1\n",
    "\n",
    "                else:\n",
    "                    df_dict[key][line] = 1\n",
    "                            \n",
    "        for elem in df_dict[key]:            \n",
    "            df_dict[key][elem] = count / df_dict[key][elem]\n",
    "            df_dict[key][elem] = np.log(df_dict[key][elem])\n",
    "            \n",
    "    print(\"df finished\")\n",
    "    \n",
    "    vector_set = {}\n",
    "    the_dict = {}\n",
    "    \n",
    "    for key in traingroups_titledata:\n",
    "        the_dict[key] = {}\n",
    "        for data in traingroups_titledata[key]:\n",
    "            for line in data[1].split():\n",
    "                if len(line) <= 2:\n",
    "                    continue\n",
    "                elif line in the_dict[key]:\n",
    "                    the_dict[key][line] += 1\n",
    "                else:\n",
    "                    the_dict[key][line] = 1\n",
    "                            \n",
    "    for key in the_dict:\n",
    "        l = []\n",
    "        for elem in the_dict[key]:\n",
    "            l.append((the_dict[key][elem], elem))\n",
    "            \n",
    "        l = list(reversed(sorted(l)))\n",
    "        length = len(l)\n",
    "        first = 0 #length // 100\n",
    "        second = length\n",
    "        \n",
    "        l = l[first:second]\n",
    "        ll = []\n",
    "        for el in l:\n",
    "            if el[0] <= 5:\n",
    "                break\n",
    "            ll.append(el[1])\n",
    "        \n",
    "        s = set(ll)\n",
    "        vector_set[key] = {a: i for a, i in zip(ll, range(len(ll)))}\n",
    "        \n",
    "    print(\"vector set finished\")\n",
    "               \n",
    "         \n",
    "    tfidf_dict = {}\n",
    "    \n",
    "    for key in traingroups_titledata: # by groups\n",
    "        dim1 = len(traingroups_titledata[key])\n",
    "        dim2 = len(vector_set[key])\n",
    "        l = np.zeros(dim1)\n",
    "        matr = np.zeros((dim1, dim2))\n",
    "        \n",
    "        for i, elem in enumerate(traingroups_titledata[key]): # by tuple (doc_id, title, target)\n",
    "            l[i] = elem[0]\n",
    "            file = elem[1]# by matr string ~ document tf-idf\n",
    "            count = 0\n",
    "            for line in file:\n",
    "                count += 1\n",
    "                line = line[:-1]\n",
    "                if line in vector_set[key]:\n",
    "                    ind = vector_set[key][line]\n",
    "                    matr[i][ind] += 1\n",
    "                       \n",
    "            if count != 0:\n",
    "                matr[i] /= count\n",
    "\n",
    "            for j, word in enumerate(vector_set[key]):\n",
    "                matr[i][j] *= df_dict[key][word]\n",
    "                            \n",
    "        tfidf_dict[key] = (l, matr)\n",
    "    \n",
    "    print(\"tfidf finished\")\n",
    "    \n",
    "    ans = {}\n",
    "    \n",
    "    for key in tfidf_dict:\n",
    "        group_sum_headers = np.zeros(tfidf_dict[key][1].shape[1])\n",
    "        for i, elem in enumerate(traingroups_titledata[key]):\n",
    "            vec = tfidf_dict[key][1][np.argwhere(tfidf_dict[key][0] == elem[0])[0][0]]\n",
    "            group_sum_headers += vec\n",
    "        \n",
    "        cos_closest = metrics.pairwise.cosine_distances(tfidf_dict[key][1], tfidf_dict[key][1])\n",
    "        \n",
    "        ans[key] = {}\n",
    "        for i, elem in enumerate(tfidf_dict[key][0]):\n",
    "            doc_id = elem\n",
    "            ans1 = 0\n",
    "            ans2 = 0\n",
    "            ans3 = 0\n",
    "            ans4 = 0\n",
    "            ans5 = 0\n",
    "            ans6 = 0\n",
    "            vec = tfidf_dict[key][1][np.argwhere(tfidf_dict[key][0] == elem)[0][0]]\n",
    "\n",
    "            ans1 = metrics.pairwise.cosine_distances([group_sum_headers], [vec])[0][0]\n",
    "                \n",
    "            ans2 = np.min(cos_closest[np.nonzero(cos_closest)])\n",
    "            ans3 = np.max(cos_closest[np.nonzero(cos_closest)])\n",
    "            ans4 = np.mean(cos_closest[np.nonzero(cos_closest)])\n",
    "            ans5 = np.median(cos_closest[np.nonzero(cos_closest)])\n",
    "            \n",
    "            ans[key][doc_id] = (ans1, ans2, ans3, ans4, ans5)\n",
    "        \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idf finished\n"
     ]
    }
   ],
   "source": [
    "idf_dictionary = idf(\"normalized/\", traingroups_titledata)\n",
    "print('idf finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector finished\n"
     ]
    }
   ],
   "source": [
    "vector_for_tfidf = vector_components(\"normalized/\", traingroups_titledata)\n",
    "print('vector finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf finished\n"
     ]
    }
   ],
   "source": [
    "tfidf_dict, words_lin = count_tfidf_by_groups(\"normalized/\", traingroups_titledata, idf_dictionary, vector_for_tfidf)\n",
    "print('tfidf finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict1 finished\n"
     ]
    }
   ],
   "source": [
    "dict1 = cos_metric(tfidf_dict, traingroups_titledata)\n",
    "print('dict1 finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df finished\n",
      "vector set finished\n",
      "tfidf finished\n",
      "dict2 finished\n"
     ]
    }
   ],
   "source": [
    "dict1_headers = count_tfidf_by_groups_for_headers(traingroups_titledata)\n",
    "print('dict2 finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train_groups.csv')\n",
    "pair_id_mapping = {}\n",
    "for i in range(len(train_data)):\n",
    "    new_doc = train_data.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    target = new_doc['target']\n",
    "    pair_id = new_doc['pair_id']\n",
    "    pair_id_mapping[pair_id] = (doc_group, doc_id, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "M = 5\n",
    "K = 20\n",
    "X_train = np.zeros((len(pair_id_mapping), N + M + K))\n",
    "y_train = np.zeros(len(pair_id_mapping))\n",
    "for i, key in enumerate(pair_id_mapping):\n",
    "    a, b, c, d, e = dict1[pair_id_mapping[key][0]][pair_id_mapping[key][1]]\n",
    "    f, g, h, j, k = dict1_headers[pair_id_mapping[key][0]][pair_id_mapping[key][1]]\n",
    "    X_train[i][0:N] = [a, b, c, d, e]\n",
    "    X_train[i][N:N+M] = [f, g, h, j, k]\n",
    "    X_train[i][N+M:N+M+K] = words_lin[pair_id_mapping[key][0]][pair_id_mapping[key][1]]\n",
    "    y_train[i] = pair_id_mapping[key][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.00821138e-01, 3.90644029e-03, 9.99558864e-01, ...,\n",
       "        3.97296882e-04, 3.89800714e-04, 3.82304547e-04],\n",
       "       [6.55728611e-01, 3.90644029e-03, 9.99558864e-01, ...,\n",
       "        6.40922328e-04, 6.03441491e-04, 5.39724066e-04],\n",
       "       [7.47949984e-01, 3.90644029e-03, 9.99558864e-01, ...,\n",
       "        3.18587122e-04, 3.18587122e-04, 3.14839039e-04],\n",
       "       ...,\n",
       "       [7.78696003e-01, 3.94707860e-04, 9.99760166e-01, ...,\n",
       "        1.17632951e-03, 1.17265118e-03, 1.15867354e-03],\n",
       "       [5.27740920e-01, 3.94707860e-04, 9.99760166e-01, ...,\n",
       "        5.52484969e-04, 5.51749303e-04, 5.45863977e-04],\n",
       "       [8.49868818e-01, 3.94707860e-04, 9.99760166e-01, ...,\n",
       "        2.30263376e-04, 2.30263376e-04, 2.26585047e-04]])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "глядь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idf finished\n"
     ]
    }
   ],
   "source": [
    "idf_dictionary_test = idf(\"normalized/\", testgroups_titledata)\n",
    "print('idf finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector finished\n"
     ]
    }
   ],
   "source": [
    "vector_for_tfidf_test = vector_components(\"normalized/\", testgroups_titledata)\n",
    "print('vector finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf finished\n"
     ]
    }
   ],
   "source": [
    "tfidf_dict_test, words_lin_test = count_tfidf_by_groups(\"normalized/\", testgroups_titledata, idf_dictionary_test, vector_for_tfidf_test)\n",
    "print('tfidf finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict1 finished\n"
     ]
    }
   ],
   "source": [
    "dict1_test = cos_metric(tfidf_dict_test, testgroups_titledata)\n",
    "print('dict1 finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df finished\n",
      "vector set finished\n",
      "tfidf finished\n",
      "dict2 finished\n"
     ]
    }
   ],
   "source": [
    "dict1_headers_test = count_tfidf_by_groups_for_headers(testgroups_titledata)\n",
    "print('dict2 finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test_groups.csv')\n",
    "pair_id_mapping_test = {}\n",
    "for i in range(len(test_data)):\n",
    "    new_doc = test_data.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    pair_id = new_doc['pair_id']\n",
    "    pair_id_mapping_test[pair_id] = (doc_group, doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.zeros((len(pair_id_mapping_test), N + M + K))\n",
    "for i, key in enumerate(doc_id_test):\n",
    "    a, b, c, d, e = dict1_test[pair_id_mapping_test[key][0]][pair_id_mapping_test[key][1]]\n",
    "    f, g, h, j, k = dict1_headers_test[pair_id_mapping_test[key][0]][pair_id_mapping_test[key][1]]\n",
    "    X_test[i][0:N] = [a, b, c, d, e]\n",
    "    X_test[i][N:N+M] = [f, g, h, j, k]\n",
    "    X_test[i][N+M:N+M+K] = words_lin_test[pair_id_mapping_test[key][0]][pair_id_mapping_test[key][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = np.hstack((X_train, X_train_lin))\n",
    "y_train_final = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final = np.hstack((X_test, X_test_lin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_final = scaler.fit_transform(X_train_final)\n",
    "scaler = StandardScaler()\n",
    "X_test_final = scaler.fit_transform(X_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, X_test_val, y_train_val, y_test_val = train_test_split(X_train_final, y_train_final, test_size=0.35, shuffle=False)#, random_state=0, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6838046272493573"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(learning_rate=0.1, n_estimators=100, max_depth=2)\n",
    "clf.fit(X_train_val, y_train_val)\n",
    "y_pred = clf.predict(X_test_val)\n",
    "f1_score(y_test_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = []\n",
    "for learning_rate in (0.005, 0.01, 0.1, 0.2): #, 0.3, 0.4, 0.5):\n",
    "    for n_estimators in(50, 100, 200, 300, 400):\n",
    "        for max_depth in (2, 3, 4):\n",
    "            clf = GradientBoostingClassifier(learning_rate=learning_rate, n_estimators=n_estimators, max_depth=max_depth)\n",
    "            clf.fit(X_train_val, y_train_val)\n",
    "            y_pred = clf.predict(X_test_val)\n",
    "            f1 = f1_score(y_test_val, y_pred)\n",
    "            reg.append((learning_rate, n_estimators, max_depth, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('reg.csv', 'w') as f:\n",
    "    f.write('learning_rate,n_estimators,max_depth,f1\\n')\n",
    "    for tup in reg:\n",
    "        f.write('{},{},{},{}\\n'.format(tup[0],tup[1],tup[2],tup[3]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(learning_rate=0.01, max_depth=3, n_estimators=400)\n",
    "clf.fit(X_train_final, y_train_final)\n",
    "y_pred = clf.predict(X_test_final).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('submission.csv', 'w') as f:\n",
    "    f.write('pair_id,target\\n')\n",
    "    for doc_id, y in zip(doc_id_test, y_pred):\n",
    "        f.write('{},{}\\n'.format(doc_id, y))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EOF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
